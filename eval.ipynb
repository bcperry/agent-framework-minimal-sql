{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08c249be",
   "metadata": {},
   "source": [
    "# Evaluation Pipeline — Interactive Notebook\n",
    "\n",
    "This notebook walks through the full **generate traces → evaluate → upload to Foundry portal** pipeline.\n",
    "\n",
    "**Two-step workflow:**\n",
    "1. **Generate traces** — run the real agent against gold queries with configurable model/params\n",
    "2. **Evaluate & upload** — score traces (deterministic + AI judge) and push results to Foundry\n",
    "\n",
    "Each section below is self-contained. Edit the configuration cells, then run top-to-bottom."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6641dc45",
   "metadata": {},
   "source": [
    "---\n",
    "## 0. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbdd31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Ensure project root is importable\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "load_dotenv(PROJECT_ROOT / \".env\")\n",
    "\n",
    "print(f\"Project root : {PROJECT_ROOT}\")\n",
    "print(f\"Model (env)  : {os.getenv('AZURE_OPENAI_MODEL')}\")\n",
    "print(f\"Endpoint     : {os.getenv('AZURE_OPENAI_ENDPOINT', '')[:50]}...\")\n",
    "print(f\"API version  : {os.getenv('AZURE_OPENAI_API_VERSION')}\")\n",
    "print(f\"SQL conn     : {'set' if os.getenv('AZURE_SQL_CONNECTIONSTRING') else 'MISSING'}\")\n",
    "print(f\"Foundry conn : {'set' if os.getenv('AZURE_AI_PROJECT_CONNECTION_STRING') else 'MISSING'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f5f101",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Review Configuration\n",
    "\n",
    "The pipeline draws from three configuration sources:\n",
    "- **System prompts** — `config/prompts/system.yaml` (one prompt per chat profile)\n",
    "- **Tool descriptions** — `config/prompts/tools.yaml` (single source of truth for all tool guidance)\n",
    "- **Gold dataset** — `eval/datasets/sql_agent_gold_starter.jsonl` (test queries + expected tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b805fc",
   "metadata": {},
   "source": [
    "### 1a. System Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc4f6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open(\"config/prompts/system.yaml\", \"r\") as f:\n",
    "    system_config = yaml.safe_load(f)\n",
    "\n",
    "for profile_key, profile in system_config[\"profiles\"].items():\n",
    "    prompt_preview = profile[\"text\"][:200].replace(\"\\n\", \" \")\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Profile : {profile_key}\")\n",
    "    print(f\"ID      : {profile['id']}\")\n",
    "    print(f\"Version : {profile['version']}\")\n",
    "    print(f\"Preview : {prompt_preview}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d26ac6",
   "metadata": {},
   "source": [
    "### 1b. Tool Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd9dde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config/prompts/tools.yaml\", \"r\") as f:\n",
    "    tools_config = yaml.safe_load(f)\n",
    "\n",
    "for tool_def in tools_config[\"tools\"]:\n",
    "    name = tool_def[\"tool_name\"]\n",
    "    version = tool_def[\"version\"]\n",
    "    profiles = tool_def.get(\"enabled_profiles\", [])\n",
    "    desc_preview = tool_def[\"description\"].strip().split(\"\\n\")[0]\n",
    "    rules_count = len(tool_def.get(\"usage_rules\", []))\n",
    "    print(f\"  {name:<20s}  v{version}  profiles={profiles}  rules={rules_count}\")\n",
    "    print(f\"    {desc_preview}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399d97cd",
   "metadata": {},
   "source": [
    "### 1c. Gold Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc80a978",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_path = Path(\"eval/datasets/sql_agent_gold_starter.jsonl\")\n",
    "gold_rows = [json.loads(line) for line in gold_path.read_text().strip().splitlines()]\n",
    "\n",
    "print(f\"Total gold cases: {len(gold_rows)}\\n\")\n",
    "print(f\"{'Case':<10} {'Profile':<25} {'Intent':<22} {'Expected Tools'}\")\n",
    "print(\"-\" * 90)\n",
    "for row in gold_rows:\n",
    "    print(\n",
    "        f\"{row['case_id']:<10} \"\n",
    "        f\"{row['chat_profile']:<25} \"\n",
    "        f\"{row.get('intent_class', ''):<22} \"\n",
    "        f\"{row['expected_tools']}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c2c3ce",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Generate Traces (Step 1)\n",
    "\n",
    "This runs the **real agent** against every gold query using the current system prompts and tool configs.\n",
    "\n",
    "### Configuration\n",
    "\n",
    "Edit the cell below to change model, temperature, and top-p before generating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70970c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Trace generation config ──────────────────────────────────────────────────\n",
    "# Change these values to experiment with different models and parameters.\n",
    "\n",
    "TRACE_MODEL       = None       # None = use AZURE_OPENAI_MODEL from .env (gpt-4o)\n",
    "                                # Set to \"gpt-4o-mini\" or another deployment name to override\n",
    "\n",
    "TRACE_TEMPERATURE = None       # None = model default. Range: 0.0 (deterministic) to 2.0 (creative)\n",
    "\n",
    "TRACE_TOP_P       = None       # None = model default. Range: 0.0 to 1.0 (nucleus sampling)\n",
    "\n",
    "TRACE_DELAY       = 3.0        # Seconds between queries (rate-limit courtesy for S0 tier)\n",
    "\n",
    "PROFILE_FILTER    = None       # None = all profiles. Set to \"Tactical Readiness AI\" etc. for one profile\n",
    "\n",
    "GOLD_DATASET      = \"eval/datasets/sql_agent_gold_starter.jsonl\"\n",
    "TRACES_OUTPUT     = \"eval/traces/agent_runs.jsonl\"\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "print(\"Trace generation config:\")\n",
    "print(f\"  Model       : {TRACE_MODEL or os.getenv('AZURE_OPENAI_MODEL', '(not set)')}\")\n",
    "print(f\"  Temperature : {TRACE_TEMPERATURE or '(model default)'}\")\n",
    "print(f\"  Top-P       : {TRACE_TOP_P or '(model default)'}\")\n",
    "print(f\"  Delay       : {TRACE_DELAY}s\")\n",
    "print(f\"  Profile     : {PROFILE_FILTER or '(all)'}\")\n",
    "print(f\"  Gold dataset: {GOLD_DATASET}\")\n",
    "print(f\"  Output      : {TRACES_OUTPUT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4248e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Run trace generation ─────────────────────────────────────────────────────\n",
    "# This calls generate_traces.py's generate() function directly (no subprocess).\n",
    "# Takes ~2-5 minutes depending on query count and rate limits.\n",
    "\n",
    "from eval.generate_traces import generate\n",
    "\n",
    "await generate(\n",
    "    gold_path=Path(GOLD_DATASET),\n",
    "    output_path=Path(TRACES_OUTPUT),\n",
    "    profile_filter=PROFILE_FILTER,\n",
    "    inter_query_delay=TRACE_DELAY,\n",
    "    model_override=TRACE_MODEL,\n",
    "    temperature=TRACE_TEMPERATURE,\n",
    "    top_p=TRACE_TOP_P,\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Traces written to:\", TRACES_OUTPUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e2541d",
   "metadata": {},
   "source": [
    "### 2a. Inspect Generated Traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5713e95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "traces_path = Path(TRACES_OUTPUT)\n",
    "traces = [json.loads(line) for line in traces_path.read_text().strip().splitlines()]\n",
    "\n",
    "# ── Run-level config from first trace ──\n",
    "if traces:\n",
    "    _t0 = traces[0]\n",
    "    print(\"Run Config:\")\n",
    "    print(f\"  Model       : {_t0.get('model', 'n/a')}\")\n",
    "    _temp = _t0.get(\"temperature\")\n",
    "    _topp = _t0.get(\"top_p\")\n",
    "    print(f\"  Temperature : {_temp if _temp is not None else '(model default)'}\")\n",
    "    print(f\"  Top-P       : {_topp if _topp is not None else '(model default)'}\")\n",
    "    _manifest = _t0.get(\"prompt_manifest\", {})\n",
    "    if _manifest:\n",
    "        print(f\"  Prompts     : {_manifest}\")\n",
    "    print()\n",
    "\n",
    "print(f\"Traces loaded: {len(traces)}\\n\")\n",
    "print(f\"{'Case':<10} {'Profile':<25} {'Tools Used':<45} {'Response Preview'}\")\n",
    "print(\"-\" * 120)\n",
    "\n",
    "for t in traces:\n",
    "    tools_used = [e[\"name\"] for e in t.get(\"tool_events\", []) if e.get(\"name\")]\n",
    "    resp = (t.get(\"output\") or \"\")[:50].replace(\"\\n\", \" \")\n",
    "    print(\n",
    "        f\"{t.get('case_id', '?'):<10} \"\n",
    "        f\"{t.get('chat_profile', '?'):<25} \"\n",
    "        f\"{str(tools_used):<45} \"\n",
    "        f\"{resp}...\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdd13f7",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Run Evaluation & Upload to Portal (Step 2)\n",
    "\n",
    "This scores every trace using:\n",
    "- **Deterministic metrics** — required/forbidden tools, sequence, F1\n",
    "- **AI judge metrics** — IntentResolution, TaskAdherence, ToolCallAccuracy\n",
    "- **Portal upload** — results pushed to Azure AI Foundry for tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584ef378",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "from importlib import reload\n",
    "\n",
    "# Suppress noisy SDK logging that floods notebook output and breaks\n",
    "# VS Code's per-cell output routing during \"Run All\".\n",
    "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"azure\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"promptflow\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"execution\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"execution.bulk\").setLevel(logging.WARNING)\n",
    "\n",
    "import eval.foundry.run_eval as _run_eval_mod\n",
    "reload(_run_eval_mod)\n",
    "from eval.foundry.run_eval import run\n",
    "\n",
    "config_path = Path(\"eval/foundry/config.yaml\")\n",
    "results = run(config_path)\n",
    "\n",
    "# Reset logging so later cells can use it normally\n",
    "logging.getLogger(\"httpx\").setLevel(logging.INFO)\n",
    "logging.getLogger(\"azure\").setLevel(logging.INFO)\n",
    "\n",
    "sys.stdout.flush()\n",
    "sys.stderr.flush()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EVALUATION COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Rows evaluated : {results.get('counts', {}).get('evaluated_rows', '?')}\")\n",
    "print(f\"Output saved   : {results.get('paths', {}).get('output_json', 'n/a')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d6ff8b",
   "metadata": {},
   "source": [
    "### 3a. Summary Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9a4415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.stdout.flush(); sys.stderr.flush()\n",
    "\n",
    "summary = results.get(\"summary\", {})\n",
    "counts = results.get(\"counts\", {})\n",
    "\n",
    "print(\"Counts:\")\n",
    "for k, v in counts.items():\n",
    "    print(f\"  {k:<25} {v}\")\n",
    "\n",
    "print(\"\\nDeterministic Metrics:\")\n",
    "for k, v in summary.items():\n",
    "    if k == \"row_count\":\n",
    "        continue\n",
    "    if isinstance(v, float):\n",
    "        print(f\"  {k:<35} {v:.1%}\")\n",
    "    else:\n",
    "        print(f\"  {k:<35} {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23445b4b",
   "metadata": {},
   "source": [
    "### 3b. Portal Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841de88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.stdout.flush(); sys.stderr.flush()\n",
    "\n",
    "portal = results.get(\"portal_evaluation\", {})\n",
    "\n",
    "print(f\"Status : {portal.get('status', 'unknown')}\")\n",
    "print(f\"Name   : {portal.get('evaluation_name', 'n/a')}\")\n",
    "\n",
    "# ── Config metadata from traces ──\n",
    "print(f\"\\nRun Configuration (visible as Tags in Foundry portal):\")\n",
    "_traces_path = Path(results.get(\"paths\", {}).get(\"traces_jsonl\", TRACES_OUTPUT))\n",
    "if _traces_path.exists():\n",
    "    _first = json.loads(_traces_path.read_text().strip().splitlines()[0])\n",
    "    print(f\"  Model           : {_first.get('model', 'n/a')}\")\n",
    "    _temp = _first.get(\"temperature\")\n",
    "    _topp = _first.get(\"top_p\")\n",
    "    print(f\"  Temperature     : {_temp if _temp is not None else '(model default)'}\")\n",
    "    print(f\"  Top-P           : {_topp if _topp is not None else '(model default)'}\")\n",
    "    _manifest = _first.get(\"prompt_manifest\", {})\n",
    "    if _manifest:\n",
    "        print(f\"  Prompt versions :\")\n",
    "        for pk, pv in _manifest.items():\n",
    "            print(f\"    {pk}: {pv}\")\n",
    "\n",
    "studio_url = portal.get(\"studio_url\")\n",
    "if studio_url:\n",
    "    print(f\"\\nView in Foundry Portal:\")\n",
    "    print(f\"   {studio_url}\")\n",
    "else:\n",
    "    print(f\"\\nNo studio URL returned.\")\n",
    "    if portal.get(\"error\"):\n",
    "        print(f\"Error: {portal['error']}\")\n",
    "\n",
    "portal_metrics = portal.get(\"metrics\")\n",
    "if portal_metrics:\n",
    "    print(\"\\nPortal AI Judge Metrics:\")\n",
    "    for k, v in sorted(portal_metrics.items()):\n",
    "        if isinstance(v, float):\n",
    "            print(f\"  {k:<45} {v:.4f}\")\n",
    "        else:\n",
    "            print(f\"  {k:<45} {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03e9ce5",
   "metadata": {},
   "source": [
    "### 3c. Per-Row Detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3545049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.stdout.flush(); sys.stderr.flush()\n",
    "\n",
    "for row in results.get(\"rows\", []):\n",
    "    case_id = row[\"case_id\"]\n",
    "    det = row.get(\"deterministic_metrics\", {})\n",
    "    ai = row.get(\"ai_judge_metrics\", {})\n",
    "\n",
    "    req_pass = \"PASS\" if det.get(\"required_tools_pass\") else \"FAIL\"\n",
    "    forb_pass = \"PASS\" if det.get(\"forbidden_tools_pass\") else \"FAIL\"\n",
    "    seq_pass = \"PASS\" if det.get(\"expected_sequence_pass\") else \"FAIL\"\n",
    "    f1 = det.get(\"tool_f1\", 0)\n",
    "\n",
    "    intent = ai.get(\"intent_resolution\", {})\n",
    "    task = ai.get(\"task_adherence\", {})\n",
    "    tool_acc = ai.get(\"tool_call_accuracy\", {})\n",
    "\n",
    "    print(f\"\\n{'─'*60}\")\n",
    "    print(f\"{case_id}: {row['query'][:70]}\")\n",
    "    print(f\"  Deterministic: req={req_pass}  forb={forb_pass}  seq={seq_pass}  F1={f1:.2f}\")\n",
    "    print(f\"  Actual tools : {det.get('actual_tools', [])}\")\n",
    "\n",
    "    if det.get(\"required_missing\"):\n",
    "        print(f\"  Missing required: {det['required_missing']}\")\n",
    "    if det.get(\"forbidden_hit\"):\n",
    "        print(f\"  Forbidden used  : {det['forbidden_hit']}\")\n",
    "\n",
    "    # AI judge scores\n",
    "    intent_score = intent.get(\"intent_resolution\", \"n/a\")\n",
    "    task_score = task.get(\"task_adherence\", \"n/a\")\n",
    "    tool_score = tool_acc.get(\"tool_call_accuracy\", \"n/a\")\n",
    "    print(f\"  AI Judge     : intent={intent_score}  task={task_score}  tool_acc={tool_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eae372d",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Compare Experiments\n",
    "\n",
    "Load and compare multiple result files side-by-side.  \n",
    "After each run, results are saved to `eval/results/foundry_eval_latest.json`.  \n",
    "To preserve a baseline, copy the file before re-running:\n",
    "\n",
    "```powershell\n",
    "Copy-Item eval/results/foundry_eval_latest.json eval/results/baseline_gpt4o_t0.json\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0c577e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── List available result files ──────────────────────────────────────────────\n",
    "results_dir = Path(\"eval/results\")\n",
    "result_files = sorted(results_dir.glob(\"*.json\"))\n",
    "\n",
    "print(\"Available result files:\")\n",
    "for f in result_files:\n",
    "    print(f\"  {f.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f6c5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Compare two result files ──────────────────────────────────────────────────\n",
    "# Edit these paths to point to the two runs you want to compare.\n",
    "\n",
    "FILE_A = \"eval/results/foundry_eval_latest.json\"  # e.g., baseline\n",
    "FILE_B = None  # Set to a second file path to compare, e.g.:\n",
    "# FILE_B = \"eval/results/baseline_gpt4o_t0.json\"\n",
    "\n",
    "\n",
    "def _load_result(path_str):\n",
    "    if not path_str:\n",
    "        return None\n",
    "    p = Path(path_str)\n",
    "    if not p.exists():\n",
    "        print(f\"  File not found: {p}\")\n",
    "        return None\n",
    "    return json.loads(p.read_text())\n",
    "\n",
    "\n",
    "def _print_run(label, run_data, file_name):\n",
    "    \"\"\"Print config + metrics for a single run.\"\"\"\n",
    "    print(f\"{label}: {file_name}\")\n",
    "    print(f\"  Timestamp  : {run_data.get('timestamp_utc', 'n/a')}\")\n",
    "    print(f\"  Rows       : {run_data.get('counts', {}).get('evaluated_rows', '?')}\")\n",
    "\n",
    "    # Config metadata from traces\n",
    "    traces_file = run_data.get(\"paths\", {}).get(\"traces_jsonl\")\n",
    "    if traces_file and Path(traces_file).exists():\n",
    "        _t0 = json.loads(Path(traces_file).read_text().strip().splitlines()[0])\n",
    "        _temp = _t0.get(\"temperature\")\n",
    "        _topp = _t0.get(\"top_p\")\n",
    "        print(f\"  Model      : {_t0.get('model', 'n/a')}\")\n",
    "        print(f\"  Temperature: {_temp if _temp is not None else '(default)'}\")\n",
    "        print(f\"  Top-P      : {_topp if _topp is not None else '(default)'}\")\n",
    "        _manifest = _t0.get(\"prompt_manifest\", {})\n",
    "        if _manifest:\n",
    "            for pk, pv in _manifest.items():\n",
    "                print(f\"  prompt.{pk}: {pv}\")\n",
    "\n",
    "    s = run_data.get(\"summary\", {})\n",
    "    print(f\"  Req pass   : {s.get('required_tools_pass_rate', 0):.1%}\")\n",
    "    print(f\"  Forb pass  : {s.get('forbidden_tools_pass_rate', 0):.1%}\")\n",
    "    print(f\"  Seq pass   : {s.get('expected_sequence_pass_rate', 0):.1%}\")\n",
    "    print(f\"  Avg F1     : {s.get('avg_tool_f1', 0):.2f}\")\n",
    "\n",
    "    portal = run_data.get(\"portal_evaluation\", {})\n",
    "    pm = portal.get(\"metrics\", {})\n",
    "    if pm:\n",
    "        for k in sorted(pm):\n",
    "            print(f\"  {k}: {pm[k]}\")\n",
    "\n",
    "\n",
    "run_a = _load_result(FILE_A)\n",
    "run_b = _load_result(FILE_B)\n",
    "\n",
    "if run_a:\n",
    "    _print_run(\"Run A\", run_a, Path(FILE_A).name)\n",
    "\n",
    "if run_b:\n",
    "    print()\n",
    "    _print_run(\"Run B\", run_b, Path(FILE_B).name)\n",
    "\n",
    "if run_a and run_b:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"DELTA (B - A):\")\n",
    "    sa = run_a.get(\"summary\", {})\n",
    "    sb = run_b.get(\"summary\", {})\n",
    "    for key in [\"required_tools_pass_rate\", \"forbidden_tools_pass_rate\", \"expected_sequence_pass_rate\", \"avg_tool_f1\"]:\n",
    "        va = sa.get(key, 0)\n",
    "        vb = sb.get(key, 0)\n",
    "        delta = vb - va\n",
    "        arrow = \"↑\" if delta > 0 else \"↓\" if delta < 0 else \"=\"\n",
    "        print(f\"  {key:<35} {delta:+.4f}  {arrow}\")\n",
    "elif not run_b:\n",
    "    print(\"\\nSet FILE_B to a second result file to see a comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceef3a92",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Quick-Reference: Editing Prompts & Tools\n",
    "\n",
    "### System Prompts (`config/prompts/system.yaml`)\n",
    "\n",
    "Each profile has a `text` field with the system prompt and a `version` field.  \n",
    "To iterate:\n",
    "1. Edit the `text` under the profile you want to change\n",
    "2. Bump the `version` (e.g., `1.0.0` → `1.1.0`)\n",
    "3. Re-run **Section 2** (generate traces) then **Section 3** (evaluate)\n",
    "\n",
    "### Tool Descriptions (`config/prompts/tools.yaml`)\n",
    "\n",
    "Each tool has `description`, `usage_rules`, and `examples`.  \n",
    "These are the **single source of truth** — changes here flow to both the agent at runtime and the evaluator.\n",
    "\n",
    "### Gold Dataset (`eval/datasets/sql_agent_gold_starter.jsonl`)\n",
    "\n",
    "One JSON line per test case. Required fields:\n",
    "```json\n",
    "{\n",
    "  \"case_id\": \"SQL-011\",\n",
    "  \"chat_profile\": \"Tactical Readiness AI\",\n",
    "  \"query\": \"Your test question\",\n",
    "  \"expected_tools\": [\"list_views\", \"describe_table\", \"read_query\"],\n",
    "  \"required_tools\": [\"read_query\"],\n",
    "  \"forbidden_tools\": [\"semantic_search\"]\n",
    "}\n",
    "```\n",
    "\n",
    "### Model Parameters\n",
    "\n",
    "Edit `TRACE_MODEL`, `TRACE_TEMPERATURE`, `TRACE_TOP_P` in **Section 2** config cell.  \n",
    "Each trace records which model/params produced it, so you can correlate scores with configuration.\n",
    "\n",
    "### Tracking in Foundry Portal\n",
    "\n",
    "Every evaluation run creates a uniquely-named entry (e.g., `foundry-eval-20260223-211910`).  \n",
    "In the portal under **Build → Evaluation**, select multiple runs to compare side-by-side."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4aac284",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. File Map\n",
    "\n",
    "| File | Purpose |\n",
    "|------|--------|\n",
    "| `config/prompts/system.yaml` | System prompts per profile |\n",
    "| `config/prompts/tools.yaml` | Tool descriptions & usage rules (single source of truth) |\n",
    "| `eval/datasets/sql_agent_gold_starter.jsonl` | Gold test cases with expected tools |\n",
    "| `eval/generate_traces.py` | Headless agent runner (Step 1) |\n",
    "| `eval/foundry/run_eval.py` | Evaluation scorer + portal upload (Step 2) |\n",
    "| `eval/foundry/config.yaml` | Eval runner config (paths, metrics, Azure settings) |\n",
    "| `eval/traces/agent_runs.jsonl` | Generated traces (output of Step 1) |\n",
    "| `eval/results/foundry_eval_latest.json` | Full local results (output of Step 2) |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
